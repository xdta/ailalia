# ailalia

PROJECT SITE W/ ALL AUDIOS: https://xdta.github.io/ailalia/

I will add some more audio files and documentation later tonight, but the basics are here!

![Slide1](https://github.com/user-attachments/assets/8d6b4012-f4ed-4a51-8bcc-2dafafad079c)
> I put together these slides in part to think through my process and organize content, so I'll kind of "annotate" these slides.

![Slide2](https://github.com/user-attachments/assets/be257c38-1644-440c-b1b7-11c4b576cb2e)
> I wrote this slide because I realized often when I was thinking of possible projects, I came up with ideas but then took a step and realized--wait, I could totally do this in another way that doesn't involve AI. 

![Slide3](https://github.com/user-attachments/assets/9c38f3a1-1632-4323-90d2-eeee10c4dc95)
> I didn't want to shove AI into the equation for the sake of the assignment. I wanted AI to be meaningfully integrated and explored. The process of doing so also led me to synthesize some reflections about the use of genAI in creative practices.

![Slide4](https://github.com/user-attachments/assets/b65df095-7b1f-4077-bb40-20e51ac35b02)
> At a basic level, I see in my research on how creative technologists are using generative AI that a lot of usage is minimally reflected directly in the final product. Sometimes, I ask if they use generative AI as part of their creative tech workflow and people will say "no," because they envision a more direct intervention.
 
![Slide5](https://github.com/user-attachments/assets/29a52fac-fb0e-499a-bc1c-aa554a8b176e)
> One additional motivation is my interest in sociolinguistics.

![Slide6](https://github.com/user-attachments/assets/8bee296d-4187-4a91-948d-d5737a4a730a)
> One thing I forgot to mention in the presentation slides is that I actually also used an LLM to generate a script in Leshanhua (in addition to versions in Chengdu dialect and Sichuanese Mandarin), which I then gave to my dad to recite. Even though my dad has lost a bit of his Leshanese pronunciation from constant migrations (truth be told, I don't think he speaks any dialect/language completely "accurately," and he often confuses speakers of every language, but he confirmed that the scripts used appropriate vocabulary. I tried to use voice to voice from my dad’s voice to my voice clone with Audimee and found that it didn’t seem to incorporate my voice well (it sounded nothing like me even after recursing on this process a couple times)

![Slide7](https://github.com/user-attachments/assets/a5620336-43a1-4886-ada0-0662d42a9764)
> This ongoing project was another big reason that I decided to look more into voice cloning software. As I've mentioned on Discord previously, I'm helping with a project to digitally bring the voice and image of a friend's friend into some old footage. His idiolect is highly distinctive in terms of word choice, prosody, intonation/pitch. This project helped me better grasp a variety of voice cloning tools and methods, establish key points of the workflow (cleaning, diarizing, sample preparation, model training, TTS generation vs Voice to Voice, stitching into dialogue), and a working idea of various models' performance based on repeated trial and error (a lot of this feels very subjective). I think, on the whole, Fish Audio performed better than non-professional Elevenlabs voice cloning.
 
![Slide8](https://github.com/user-attachments/assets/ecec1c6a-6c95-43ef-b317-41eaaa29d9db)
> The idea for this came to me, and something appealed to me about playing around with the idea of "progenation" in carbon or silicon form. In what ways do we see/hear traces of the originating form?

![Slide9](https://github.com/user-attachments/assets/b8a52219-cbb7-4388-adb5-9f3aafbb14ff)
> Another key experiment I wanted to run was with singing/vocals. One key factor to note with cloning vocals was the need for artistic control after generation (e.g. not only which pitches but vocal texture/ornamentation). Given that I can be pretty versatile within my range (~B2 – E5), I think I was more interested in contexts in which individual characteristics were simultaneously present and obscured—perhaps why I was drawn to the Harmony and Mixed Vocals options. I think I’m still not sure in what ways this would be a sustainable (not environmentally here…) practice (beyond more one-off experiment), but I think we are still grappling with the affordances of AI as a medium, as with e.g. photography, textiles, paint.

![Slide10](https://github.com/user-attachments/assets/b35eae49-e04c-4b6d-b906-501c53b95fc7)
>To be honest, I thought it would be pretty funny to create a ceramic alcohol container of some kind containing a qr code at the bottom (once the drinker finished imbibing) that led to a religious chant, but I didn’t get to do more than my earliest 3D QR code on ceramic object tests. I was inspired to try this experiment because, in ruminating about the questions driving this project, I thought about how computation handles repetition well. When does it no longer become sensible to offload repetition to the machine? Religious robots (e.g. the various forms in Japan) are fascinating. 
Just a quick note about this process too: it consists of 1) generating a QR code, 2) extruding the QR code into a 3d-printed stamp, 3) stamping that onto wet clay, 4) bisque firing, 5) underglaze inlay, 6) glaze firing. Thin clay (which is the use case I tested) is HIGHLY likely to warp, especially through two firings (also because, and clay changes size/shape during firing. I was able to get the QR code on porcelain clay to work after two firings, but there are some interesting ways to perhaps optimize this through clay and firing choices.

<p align="center">
  <img src="https://github.com/user-attachments/assets/f1285311-662e-437e-b4cf-49f2102e5ca0" alt="Screen_Recording_20250429_003057_Camera-ezgif com-video-to-gif-converter">
  <img src="https://github.com/user-attachments/assets/73a96b77-9f1d-4570-9d76-926ff794e679" width="240">
  <img src="https://github.com/user-attachments/assets/274b2aa2-01d5-4261-970a-ec1fd7a7085a" width="240">
</p>

<p align="center">
 <img src="https://github.com/user-attachments/assets/2471db7b-9f5c-4a9f-ad06-6975fb299315" alt="Screen_Recording_20250429_003057_Camera-ezgif com-video-to-gif-converter">
</p>

![Slide11](https://github.com/user-attachments/assets/4d6c8771-94de-4b21-b73a-e5e49e695479)
>Something that I’m consistently drawn to is testing the boundaries of what any system can do. When do these systems break down? And what do these breakdowns tell us about the nature of the system? How might we showcase these breakdowns in an interesting way? (very media theory)

>This slide represented two explorations:
> 1)	The reading of the slide heading consists of TTS generations of my Audimee mixed vocals (basically, it’s tunable mix of my cloned singing voice + an instrument—in this case, the bansuri and… This also suggests to me that there might be more explorations to be done in a live performance context with generative voice.
> 2)	I was interested in inputting intentionally weird data and seeing how that would affect output. This is made difficult by the fact that a lot of voice cloning software simply modifies an existing voice instead of creating a new one from scratch; they’re too “perfect”0
> 3)	One interesting value that Golan also mentioned during our conversations was the ability of the machine to consistently do something odd that’s hard for us to sustain (e.g. gurgling water, belching, etc). That’s definitely an area I want to continue to problem more systematically

> P.S. Still miffed about how I forgot about the lag with the projection system and it was hard to time the audio with the presentation.

![Slide12](https://github.com/user-attachments/assets/102fa65c-6729-40d5-9d15-1502142bbb88)
> Finally, there are actually a couple more projects I already have in mind to probe
> 1) I think another possibility of AI, as hinted by our project to “resurrect” Hiroya, is to preserve. I want to clone the voices of animals known to be going extinct and create a symphony from these voices.
> 2) I want to try harmonizing more with generated voices.


![Slide13](https://github.com/user-attachments/assets/75ab851b-fdab-4f29-a28e-69d01709d318)
> Truly thanks again to everyone for the loveliest of semesters.  The community, the interests/passions, the energy/vibes—truly make a world of difference. Appreciate your dedication to teaching, Em and Golan!


